{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.bencmark = True\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os,sys,cv2,random,datetime,time,math\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from net_s3fd import *\n",
    "#from s3fd_gender import *\n",
    "#import s3fd_gender\n",
    "from s3fd import *\n",
    "import s3fd\n",
    "from bbox import *\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CelebDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping images and target labels\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "        PIL transforms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, img_path, img_ext, transform=None):\n",
    "    \n",
    "        tmp_df = pd.read_csv(csv_path)\n",
    "        assert tmp_df['Image_Name'].apply(lambda x: os.path.isfile(img_path + x + img_ext)).all(), \\\n",
    "\"Some images referenced in the CSV file were not found\"\n",
    "        \n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.img_path = img_path\n",
    "        self.img_ext = img_ext\n",
    "        self.transform = transform\n",
    "\n",
    "        self.X_train = tmp_df['Image_Name']\n",
    "        #gender training\n",
    "        self.y_train = self.mlb.fit_transform(tmp_df['Gender'].str.split()).astype(np.float32)\n",
    "     \n",
    "    #resizing the input image\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.img_path + self.X_train[index] + self.img_ext)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        #img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        #img = Variable(torch.from_numpy(img).float(),volatile=True)\n",
    "        \n",
    "        #if self.transform is not None:\n",
    "        #    img = self.transform(img)\n",
    "        \n",
    "        #label === gender \n",
    "        label = torch.from_numpy(self.y_train[index])\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformations = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor()\n",
    "     \n",
    "     #transforms.Normalize(mean=[104,117,123])\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = \"index.csv\"\n",
    "img_path = \"data/Celeb_Small_Dataset/\"\n",
    "img_ext = \".jpg\"\n",
    "dset = CelebDataset(train_data,img_path,img_ext,transformations)\n",
    "train_loader = DataLoader(dset,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1 # 1 for CUDA\n",
    "                         # pin_memory=True # CUDA only\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save(model, optimizer, loss, filename):\n",
    "    save_dict = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.data[0]\n",
    "        }\n",
    "    torch.save(save_dict, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_classes, num_epochs = 100):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i,(img,label) in enumerate(train_loader):\n",
    "            img = img.view((1,)+img.shape[1:])\n",
    "            genlist = []\n",
    "            #loss = torch.tensor(1,2)\n",
    "            #loss = []\n",
    "                        \n",
    "            #if use_cuda:\n",
    "            #    data, target = Variable(img.cuda()), Variable(torch.Tensor(label).cuda())\n",
    "            #else:\n",
    "            data, target = Variable(img), Variable(torch.Tensor(label))\n",
    "            target = target.view(num_classes,1)\n",
    "            \n",
    "            #output as desired form\n",
    "            \n",
    "            \n",
    "            #apply soft max to gender layers\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #run through model\n",
    "            outputs = model(data)\n",
    "            #print(outputs)\n",
    "                \n",
    "            #apply softmax to layers\n",
    "            for j in range(len(outputs)//2): outputs[j*2] = F.softmax(outputs[j*2])\n",
    "            #fetch data\n",
    "            for j in range(len(outputs)//2): \n",
    "                ocls,ogen = outputs[i*2].data.cpu(),outputs[i*2+1].data.cpu()\n",
    "                #print(ocls)\n",
    "                \n",
    "                FB,FC,FH,FW = ocls.size() # feature map size\n",
    "                print(FH, FW)\n",
    "                stride = 2**(i+2)    # 4,8,16,32,64,128\n",
    "                anchor = stride*4\n",
    "                for Findex in range(FH*FW):\n",
    "                    windex,hindex = Findex%FW,Findex//FW\n",
    "                    axc,ayc = stride/2+windex*stride,stride/2+hindex*stride\n",
    "                    score = ocls[0,1,hindex,windex]\n",
    "                    #loc = oreg[0,:,hindex,windex].contiguous().view(1,4)\n",
    "                    if score<0.05: continue\n",
    "                    gen = ogen[0,:,hindex,windex].contiguous().view(1,2)\n",
    "                    print(gen)\n",
    "                    #Append this gender prediction scores (1x2 dimension) to a list.\n",
    "                    #genlist.append(gen)\n",
    "                    #loss += criterion(gen,target)\n",
    "                    genlist.append(gen)\n",
    "                  \n",
    "            #For each of the predictions in gender prediction list: \n",
    "            #calculate the loss and add it to the total loss for this iteration.\n",
    "            \n",
    "            for k,(gen) in enumerate(genlist):\n",
    "                loss += criterion(gen, target)\n",
    "                \n",
    "            if i%50==0:\n",
    "                print(\"Reached iteration \",i)\n",
    "                running_loss += loss.data[0]\n",
    "                #running_loss += loss\n",
    "            \n",
    "            #Update\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #total running loss\n",
    "            running_loss += loss.data[0]\n",
    "            #running_loss += loss\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            save(model, optimizer, loss, 'faceRecog.saved.model')\n",
    "        print(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine our model with the pre-trained model\n",
    "\n",
    "num_classes = 2\n",
    "myModel = (s3fd.s3fd_original(num_classes))\n",
    "loadedModel = torch.load('s3fd_convert.pth')\n",
    "\n",
    "newModel = myModel.state_dict()\n",
    "pretrained_dict = {k: v for k, v in loadedModel.items() if k in newModel}\n",
    "\n",
    "new_params = [k for k in newModel if k not in loadedModel]\n",
    "\n",
    "newModel.update(pretrained_dict)\n",
    "myModel.load_state_dict(newModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3fd_original(\n",
       "  (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3))\n",
       "  (fc7): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv6_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv7_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv7_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv4_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv5_3_norm): L2Norm(\n",
       "  )\n",
       "  (conv3_3_norm_mbox_conf): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3_3_norm_mbox_loc): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4_3_norm_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv5_3_norm_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_conf): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_loc): Conv2d(1024, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc7_mbox_gender): Conv2d(1024, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_conf): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_loc): Conv2d(512, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6_2_mbox_gender): Conv2d(512, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2_mbox_conf): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv7_2_mbox_loc): Conv2d(256, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if using GPU\n",
    "use_cuda = True\n",
    "myModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 3, 3])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = myModel.conv4_3_norm_mbox_conf.weight[0][None,:,:,:]\n",
    "temp2 = myModel.conv5_3_norm_mbox_conf.weight[0][None,:,:,:]\n",
    "temp3 = myModel.fc7_mbox_conf.weight[0][None,:,:,:]\n",
    "temp4 = myModel.conv6_2_mbox_conf.weight[0][None,:,:,:]\n",
    "\n",
    "temp1 = torch.cat((temp1,temp1),0)\n",
    "temp2 = torch.cat((temp2,temp2),0)\n",
    "temp3 = torch.cat((temp3,temp3),0)\n",
    "temp4 = torch.cat((temp4,temp4),0)\n",
    "temp3.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99\n",
      "----------\n",
      "f3_3\n",
      "torch.Size([1, 256, 32, 32])\n",
      "f4_3\n",
      "torch.Size([1, 512, 16, 16])\n",
      "f5_3\n",
      "torch.Size([1, 512, 8, 8])\n",
      "ffc7\n",
      "torch.Size([1, 1024, 12, 12])\n",
      "f6_2\n",
      "torch.Size([1, 512, 6, 6])\n",
      "f7\n",
      "torch.Size([1, 256, 3, 3])\n",
      "resized_cls1\n",
      "torch.Size([1, 2, 64, 64])\n",
      "32 32\n",
      "32 32\n",
      "32 32\n",
      "32 32\n",
      "Reached iteration  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-d1a0e2933740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Call the training function defined above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-89453be94aa5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_classes, num_epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reached iteration \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0;31m#running_loss += loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.BCEloss()\n",
    "\n",
    "\n",
    "# Turn the requires_grad off for all the parameters in your model.\n",
    "# except the fc layers, ****************\n",
    "#for param in myModel.parameters():\n",
    "#    if param in myModel.fc7.parameters():\n",
    "#        param.requires_grad = True \n",
    "#    else:\n",
    "#        param.requires_grad = False\n",
    "\n",
    "for pair in myModel.named_parameters():\n",
    "    if pair[0] not in new_params:\n",
    "        pair[1].requires_grad = False\n",
    "        \n",
    "        \n",
    "# myModel.conv4_3_norm_mbox_conf.weight[0] += genTensor4_3\n",
    "# Add this new tensor to the Gender layer's weight. \n",
    "# Make sure it has dimension N x C x H x W = N x 2 x H x W. (N is the number of images in the batch)\n",
    "\n",
    "\n",
    "#Stochastic Gradient Descent.\n",
    "#optimizer = optim.SGD(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad,myModel.parameters()), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "#if use_cuda: \n",
    "#    myModel = myModel.cuda()\n",
    "\n",
    "# Call the training function defined above.    \n",
    "model_ft = train_model(myModel, criterion, optimizer, num_classes, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test the input and output\n",
    "\n",
    "def transform(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img - np.array([104,117,123])\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        img = img.reshape((1,)+img.shape)\n",
    "        img = torch.from_numpy(img).float()\n",
    "        \n",
    "        return Variable(img.cuda())\n",
    "\n",
    "    \n",
    "myModel = myModel.cuda()\n",
    "testImage1 = transform('data/Test/TestCeleb_4/25-FaceId-0.jpg')\n",
    "testImage2 = transform('data/Test/TestCeleb_4/26-FaceId-0.jpg')\n",
    "testImage3 = transform('data/Test/TestCeleb_4/27-FaceId-0.jpg')\n",
    "testImage4 = transform('data/Test/TestCeleb_10/25-FaceId-0.jpg')\n",
    "testImage5 = transform('data/Test/TestCeleb_10/26-FaceId-0.jpg')\n",
    "testImage6 = transform('data/Test/TestCeleb_10/24-FaceId-0.jpg')\n",
    "\n",
    "output1 = myModel(testImage1)\n",
    "output2 = myModel(testImage2)\n",
    "output3 = myModel(testImage2)\n",
    "output4 = myModel(testImage4)\n",
    "output5 = myModel(testImage5)\n",
    "output6 = myModel(testImage6)\n",
    "print(\"testImage1 - \",output1)\n",
    "print(\"testImage2 - \",output2)\n",
    "print(\"testImage3 - \",output3)\n",
    "print(\"testImage1 - \",output4)\n",
    "print(\"testImage2 - \",output5)\n",
    "print(\"testImage3 - \",output6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
